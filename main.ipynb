{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60aa9a6b",
   "metadata": {},
   "source": [
    "# Does negativity make success?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dce79cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "from collections import Counter\n",
    "from helpers import text_from_ids, neg_words\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.stem import *\n",
    "import re \n",
    "import gensim\n",
    "import pickle \n",
    "import operator\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96434186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install missing libraries (uncomment lines to install)\n",
    "#!pip install wordcloud\n",
    "#!pip install pyldavis\n",
    "#!pip install --upgrade gensim\n",
    "#nltk.download('wordnet')\n",
    "#!pip install jupyter_dash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4634c7",
   "metadata": {},
   "source": [
    "#### Load the dataframe containing videos from 2019 and their features\n",
    "\n",
    "This is a big file (860 MB) so we have stored it on Google Drive. Download it from the link below and storie it as `generated/2019/2019_videos_Typo_Emojis_NegWords_Sentiment_title_desc.parquet`. It was generated by the notebook `data_processing.ipynb`.\n",
    "\n",
    "https://drive.google.com/file/d/1RmVSw2MBq0Ps0dwcTQjqZsDAuivXbUaZ/view?usp=share_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332adf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'generated/2019/2019_videos_Typo_Emojis_NegWords_Sentiment_title_desc.parquet'\n",
    "videos = pd.read_parquet(filepath, engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1c2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb853482",
   "metadata": {},
   "source": [
    "## What is negativity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbab2fa",
   "metadata": {},
   "source": [
    "### Small intro (examples with sia from vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef27211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentences = [\n",
    "    'You will never be a great YouTuber! You are lazy!',\n",
    "    'We will bring so much you fame and success! You are great!',\n",
    "    'We might or might not be able to help you.'\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print('Sentence:')\n",
    "    print(sentence)\n",
    "    print('Vader sentiment analysis:')\n",
    "    print(sia.polarity_scores(sentence))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470196db",
   "metadata": {},
   "source": [
    "# Is there something to investigate ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b640b",
   "metadata": {},
   "source": [
    "The trending videos dataset is used to determine a treshold on views and likes to split our dataset in two: Successful and not successful videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b272e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading of the trending datasets (US trending videos from 2018) [[put correct link/year]]\n",
    "df_trends = pd.read_csv('data/additionnal/US_youtube_trending_data.csv')\n",
    "df_trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f275329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determination of the tresholds\n",
    "treshold_views = np.quantile(df_trends['view_count'], 0.4)\n",
    "treshold_likes = np.quantile(df_trends['likes'], 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda096b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset of the successful videos\n",
    "videos_success = videos.loc[(videos['view_count'] > treshold_views)]\n",
    "videos_success = videos_success.loc[(videos_success['like_count'] > treshold_likes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f1809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset of the not successful videos\n",
    "new_videos = videos.drop(videos_success.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55029159",
   "metadata": {},
   "source": [
    "The two dataset need to be modified in order to plot the comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d3bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_neg_title_success = videos_success[['sia_negative_title', 'categories']]\n",
    "sia_neg_description_success = videos_success[['sia_negative_description', 'categories']]\n",
    "\n",
    "\n",
    "sia_neg_title = new_videos[['sia_negative_title', 'categories']]\n",
    "sia_neg_description = new_videos[['sia_negative_description', 'categories']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d7720",
   "metadata": {},
   "source": [
    "Construction of the negative title success dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba6f14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_title_suc = ['Negative sentiment in title']\n",
    "sia_bool_neg_title_suc = ['successful']\n",
    "\n",
    "neg_title_suc = neg_title_suc*len(sia_neg_title_success)\n",
    "sia_bool_neg_title_suc = sia_bool_neg_title_suc*len(sia_neg_title_success)\n",
    "neg_title_suc_cat = sia_neg_title_success['categories']\n",
    "sia_neg_title_success = sia_neg_title_success['sia_negative_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717920ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_neg_title_success_values = sia_neg_title_success.values\n",
    "sia_neg_title_success = sia_neg_title_success_values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe7ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of the negative title success dataset\n",
    "data_given = {'Type of sentiment': neg_title_suc, 'SIA value': sia_neg_title_success, \"Video's success\": sia_bool_neg_title_suc, 'categories':neg_title_suc_cat}\n",
    "\n",
    "neg_title_suc_df = pd.DataFrame(data = data_given)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567d5323",
   "metadata": {},
   "source": [
    "Construction of the negative title not success dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f04c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_title = ['Negative sentiment in title']\n",
    "sia_bool_neg_title = ['not successful']\n",
    "\n",
    "neg_title = neg_title*len(sia_neg_title)\n",
    "sia_bool_neg_title = sia_bool_neg_title*len(sia_neg_title)\n",
    "neg_title_cat = sia_neg_title['categories']\n",
    "sia_neg_title = sia_neg_title['sia_negative_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e013f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_neg_title_values = sia_neg_title.values\n",
    "sia_neg_title = sia_neg_title_values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of the negative title success dataset\n",
    "data_given = {'Type of sentiment': neg_title, 'SIA value': sia_neg_title, \"Video's success\": sia_bool_neg_title, 'categories':neg_title_cat}\n",
    "\n",
    "neg_title_df = pd.DataFrame(data = data_given)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b5400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total dataset for the title (merge success and not)\n",
    "neg_title_total_df = neg_title_suc_df.append(neg_title_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021e9563",
   "metadata": {},
   "source": [
    "Construction of the negative description success dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b38fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_description_suc = ['Negative sentiment in description']\n",
    "sia_bool_neg_description_suc = ['successful']\n",
    "\n",
    "neg_description_suc = neg_description_suc*len(sia_neg_description_success)\n",
    "sia_bool_neg_description_suc = sia_bool_neg_description_suc*len(sia_neg_description_success)\n",
    "neg_description_suc_cat = sia_neg_description_success['categories']\n",
    "sia_neg_description_success = sia_neg_description_success['sia_negative_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f56788",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_neg_description_success_values = sia_neg_description_success.values\n",
    "sia_neg_description_success = sia_neg_description_success_values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232af7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of the negative description success dataset\n",
    "data_given = {'Type of sentiment': neg_description_suc, 'SIA value': sia_neg_description_success, \"Video's success\": sia_bool_neg_description_suc, 'categories':neg_description_suc_cat}\n",
    "\n",
    "neg_description_suc_df = pd.DataFrame(data = data_given)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e65fe97",
   "metadata": {},
   "source": [
    "Construction of the negative description not success dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21133213",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_description = ['Negative sentiment in description']\n",
    "sia_bool_neg_description = ['not successful']\n",
    "\n",
    "neg_description = neg_description*len(sia_neg_description)\n",
    "sia_bool_neg_description = sia_bool_neg_description*len(sia_neg_description)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_description_cat = sia_neg_description['categories']\n",
    "sia_neg_description = sia_neg_description['sia_negative_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f5b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_neg_description_values = sia_neg_description.values\n",
    "sia_neg_description = sia_neg_description_values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bcd3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of the negative description not success dataset\n",
    "data_given = {'Type of sentiment': neg_description, 'SIA value': sia_neg_description, \"Video's success\": sia_bool_neg_description, 'categories':neg_description_cat}\n",
    "\n",
    "neg_description_df = pd.DataFrame(data = data_given)\n",
    "\n",
    "\n",
    "#Total dataset for the description (merge success and not)\n",
    "neg_description_total_df = neg_description_suc_df.append(neg_description_df, ignore_index=True)\n",
    "\n",
    "#Creation of the complete dataset to be plot\n",
    "neg_total_df = neg_title_total_df.append(neg_description_total_df, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23881ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.barplot(\n",
    "    data=neg_total_df, \n",
    "    x=\"Type of sentiment\", \n",
    "    y=\"SIA value\", \n",
    "    hue=\"Video's success\", \n",
    "    color='blue', \n",
    "    palette='hls',\n",
    "    n_boot=100\n",
    ")\n",
    "plt.title(\"Negative sentiment intensity in successful vs. not successful videos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc51fb51",
   "metadata": {},
   "source": [
    "## Regression analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6687522",
   "metadata": {},
   "source": [
    "### Overall\n",
    "\n",
    "try description and title (look at R to find what is best) [[Djian: description is better]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f2ed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_regression(data, formula):\n",
    "    model = smf.ols(formula=formula, data=data)\n",
    "    np.random.seed(2)\n",
    "    results = model.fit()\n",
    "    print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c307e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove videos where 'like_count' is NaN\n",
    "videos = videos[videos['like_count'].isna() == False]\n",
    "\n",
    "# Convert some rows to float\n",
    "videos['like_count'] = videos['like_count'].astype(float)\n",
    "videos['dislike_count'] = videos['dislike_count'].astype(float)\n",
    "videos['view_count'] = videos['view_count'].astype(float)\n",
    "\n",
    "# New columns for the log of the counts (+1 so that the log is always defined)\n",
    "videos['log_view_count'] = np.log(videos['view_count'] + 1)\n",
    "videos['log_like_count'] = np.log(videos['like_count'] + 1)\n",
    "videos['log_dislike_count'] = np.log(videos['dislike_count'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c8f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_factors = ['log_view_count', 'log_like_count', 'log_dislike_count']\n",
    "\n",
    "\n",
    "def regression_formula(success_factor):\n",
    "    f = f'{success_factor} ~ '\n",
    "    f += 'sia_negative_description ' \n",
    "    f += '+ sia_positive_description '\n",
    "    f += '+ sia_neutral_description '\n",
    "    return f\n",
    "\n",
    "\n",
    "formulas = [regression_formula(s) for s in success_factors]\n",
    "\n",
    "for f in formulas:\n",
    "    print(f'Regression analysis for formula \\n{f}')\n",
    "    print_regression(data=videos, formula=f)\n",
    "    print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd104b1",
   "metadata": {},
   "source": [
    "### By category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e465e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the categories\n",
    "categories = set(videos['categories'].values)\n",
    "categories.remove(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40e0667",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_for_success_factor = dict()\n",
    "\n",
    "for success_factor in success_factors:\n",
    "    f = regression_formula(success_factor)\n",
    "    \n",
    "    results_params_f = dict()\n",
    "\n",
    "    for category in categories:\n",
    "        videos_category = videos[videos['categories'] == category]\n",
    "        model = smf.ols(formula=f, data=videos_category)\n",
    "        np.random.seed(2)\n",
    "        results = model.fit()\n",
    "        results_params_f[category] = pd.concat([results.params, results.pvalues], keys=['parameter', 'p-value'])\n",
    "\n",
    "    df_regression = pd.DataFrame(results_params_f).transpose()\n",
    "    \n",
    "    regression_for_success_factor[success_factor] = df_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the p-values to check the statistical significance of the parameters\n",
    "for name, df_reg in regression_for_success_factor.items():\n",
    "    pvalues = df_reg['p-value']\n",
    "    \n",
    "    print(f'P-values for the regression parametersfor {name}')\n",
    "    print(pvalues.max())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cb4a20",
   "metadata": {},
   "source": [
    "**Result**. The regressions log_like_count and log_dislike_count are fine, but we should investigate more closely log_view_count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02dd926",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_for_success_factor['log_view_count']['p-value']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2febafd7",
   "metadata": {},
   "source": [
    "**Result.** The only coefficient with a p-value larger than 0.05 is sia_positive_description for the category Film & Animation. So this parameter is not statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c0a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression(df_regression, name=''):\n",
    "    \n",
    "    df_reg = df_regression.copy()\n",
    "    \n",
    "    # Drop p-values and `Intercept`, remove index\n",
    "    df_reg = df_reg['parameter']\n",
    "    df_reg = df_reg.drop('Intercept', axis=1)\n",
    "    df_reg = df_reg.reset_index()\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    plt.scatter(x=df_reg['index'], y=df_reg['sia_negative_description'], label='negative', marker='$:($', color='crimson', s=50)\n",
    "    plt.scatter(x=df_reg['index'], y=df_reg['sia_neutral_description'], label='neutral',  marker='$:|$', color='gray', s=50)\n",
    "    plt.scatter(x=df_reg['index'], y=df_reg['sia_positive_description'], label='positive',  marker='$:)$', color='dodgerblue', s=50)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(f'Coefficients in the linear regression for {name}, by video category')\n",
    "    plt.ylabel('log_count')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'generated/plots/regression_{name}') # Save the plot to png\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e880d1bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the regression parameters for various success factors\n",
    "for success_f in success_factors:\n",
    "    plot_regression(regression_for_success_factor[success_f], name=success_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20a8d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion: categories with sia_negative large compared to sia_neutral (in the linear regression for log_view_count)\n",
    "interesting_cats = {\n",
    "    'Entertainment', 'Film & Animation', 'Education', \n",
    "    'Autos & Vehicles', 'Howto & Style', 'Music', \n",
    "    'Pets & Animals', 'Science & Technology', 'Sports',\n",
    "    'Travel & Events'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b90f12",
   "metadata": {},
   "source": [
    "Creation of the interactive plot for the log view count depending on the sia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3346bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = regression_for_success_factor['log_view_count'].copy()\n",
    "    \n",
    "# Drop p-values and `Intercept`, remove index\n",
    "df_reg = df_reg['parameter']\n",
    "df_reg = df_reg.drop('Intercept', axis=1)\n",
    "df_reg = df_reg.reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4b097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyter_dash import JupyterDash\n",
    "import dash_html_components as html\n",
    "import dash_core_components as dcc\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import dash\n",
    "import math\n",
    "from dash import Dash, dcc, html, Input, Output\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bd7a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = JupyterDash(__name__)\n",
    "\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H4('Difference in the SIA value in the title depending on the success of the video'),\n",
    "    dcc.Dropdown(\n",
    "        id=\"dropdown\",\n",
    "        options=['People & Blogs', 'Science & Technology', 'Gaming',\n",
    "       'Film & Animation', 'News & Politics', 'Howto & Style', 'Music',\n",
    "       'Entertainment', 'Education', 'Comedy', 'Travel & Events',\n",
    "       'Sports', 'Autos & Vehicles', 'Pets & Animals',\n",
    "       'Nonprofits & Activism'],\n",
    "        value=\"People & Blogs\",\n",
    "        clearable=False,\n",
    "    ),\n",
    "    dcc.Graph(id=\"graph\"),\n",
    "])\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"graph\", \"figure\"), \n",
    "    Input(\"dropdown\", \"value\"))\n",
    "def update_bar_chart(cat):\n",
    "    df = df_reg # replace with your own data source\n",
    "    \n",
    "    mask = df[\"index\"] == cat\n",
    "    \n",
    "    inter = df_reg[mask]\n",
    "    inter_dic = {'Sentiment': ['sia_negative_description', 'sia_positive_description', 'sia_neutral_description'], 'Value': [inter['sia_negative_description'].values[0], inter['sia_positive_description'].values[0], inter['sia_neutral_description'].values[0]]}\n",
    "    inter_df = pd.DataFrame(data= inter_dic)\n",
    "\n",
    "    fig = px.bar(inter_df, x=\"Sentiment\", y=\"Value\", barmode='group')\n",
    "    return fig\n",
    "\n",
    "\n",
    "app.run_server(debug = True,port=8050)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0119c8cd",
   "metadata": {},
   "source": [
    "## What does successful negativity look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714bfff6",
   "metadata": {},
   "source": [
    "### Most used words: the vocabulary of videos that are negative and successful (for different categories) [[Maj]]\n",
    "\n",
    "Make 'histogram' of words in title/desc for videos that are very negative and have lots of success (maybe do it for each category). Example:\n",
    "\n",
    "https://ldrame21.github.io/metoo-media-impact/#data-story-title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd89f1f",
   "metadata": {},
   "source": [
    "#### What is a negative and successful video?\n",
    "\n",
    "Negative videos are the ones in the top 1% most negative videos according to sia_negative_description. \n",
    "\n",
    "Successful videos are the ones in the top 30% videos according to the number of views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c0d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos = videos.copy()\n",
    "df_videos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a241a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Whats the minimum number of views among successful videos?\n",
    "lower_bound_success = df_videos['view_count'].quantile(0.7)\n",
    "print(f'Successful videos have >= {lower_bound_success} views.')\n",
    "\n",
    "# Whats the minimum sia_negative_description value among negative videos?\n",
    "lower_bound_sia = df_videos['sia_negative_description'].quantile(0.99)\n",
    "print(f'Negative videos have sia_negative_description >= {lower_bound_sia}.')\n",
    "\n",
    "# Select negative and successful videos\n",
    "df_filtered = df_videos[(df_videos['sia_negative_description'] >= lower_bound_sia) & (df_videos['view_count'] >= lower_bound_success)]\n",
    "print(f'We have {len(df_filtered)} videos that are both successful and negative.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1053abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the tags, titles and descriptions of the videos we have filtered\n",
    "data_path = 'generated/2019/2019_videos.csv'\n",
    "video_ids = set(df_filtered['display_id'])\n",
    "df_title_des = text_from_ids(video_ids, data_path) #contains display_id, title, description and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df94da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title_des.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023e73b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge both dataframes\n",
    "df_combined = pd.merge(df_filtered, df_title_des, on=\"display_id\")\n",
    "df_combined.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27906544",
   "metadata": {},
   "source": [
    "### Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f05afb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Name of the video text field that we will use for the wordclouds ('title', 'tags' or 'description')\n",
    "field_to_analyze = 'title' \n",
    "\n",
    "# remove stopwords from the text field \n",
    "def remove_stopwords(df):\n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned['tokens'] = df_cleaned[field_to_analyze].apply(lambda title: title.split())\n",
    "    stop_words = stopwords.words('english') + ['|', '_', '-', ',', '.', '!', '?', '&', ':'] # also remove some punctuation\n",
    "    df_cleaned['tokens'] = df_cleaned['tokens'].apply(\n",
    "        lambda tokens: [token for token in tokens if token.lower() not in stop_words]\n",
    "    )\n",
    "    return df_cleaned\n",
    "\n",
    "df_cleaned = remove_stopwords(df_combined)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea531c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 5 most successful categories based on view_count (we do not use it anymore)\n",
    "'''grouped = df_cleaned.groupby(\"categories\")\n",
    "most_successful = {}\n",
    "for name, group in grouped:\n",
    "    most_successful[name] = group['view_count'].mean()\n",
    "sorted_dict = sorted(most_successful.items(), key=operator.itemgetter(1), reverse=True)[:5]\n",
    "print(sorted_dict)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be2b503",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the most common words \n",
    "common_words_with_freq = {}\n",
    "\n",
    "categorie_groups = df_cleaned.groupby(\"categories\")\n",
    "for name, group in categorie_groups:\n",
    "    flattened = [val for sublist in group['tokens'].tolist() for val in sublist]\n",
    "    common_words_with_freq[name] = Counter(flattened).most_common(100)\n",
    "#convert the dict to a dataframe   \n",
    "L = [(k, *t) for k, v in common_words_with_freq.items() for t in v]\n",
    "df_success = pd.DataFrame(L, columns=['categories','common_words','frequency'])\n",
    "df_success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56bfd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_success_all = pd.DataFrame(common_words_with_freq.items(), columns=['categories', 'most_common_words'])\n",
    "df_success_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34aed84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grouped = df_success.groupby(\"categories\")\n",
    "for name, group in grouped:\n",
    "    group_words = [item.lower() for item in grouped['common_words'].get_group(name).tolist()]\n",
    "    #group_neg_words = list(set(group_words) & neg_set)\n",
    "    if(len(group_words) != 0):\n",
    "        # Only do the wordclouds for interesting categories, that were determined manually by looking at the wordclouds\n",
    "        if name in {'Entertainment', 'Education', 'Howto & Style', 'Pets & Animals'}: \n",
    "            group_freq = grouped['frequency'].get_group(name).tolist()\n",
    "            data = dict(zip(group_words, group_freq))\n",
    "            \n",
    "            # Generate the wordcloud\n",
    "            wc = WordCloud(\n",
    "                width=800, \n",
    "                height=400, \n",
    "                max_words=250, \n",
    "                background_color='white', \n",
    "                random_state=3, # Use random_state=3 as a random seed, for reproducibilty of the results\n",
    "                #colormap='ocean',\n",
    "                mode='RGBA'\n",
    "            ).generate_from_frequencies(data)\n",
    "            \n",
    "            # Save the wordcloud to a file\n",
    "            filename = name.replace(' ', '_').replace('&', 'and') # Remove whitespaces and ampersands\n",
    "            wc.to_file(f'generated/wordclouds/wc_{filename}.png')\n",
    "\n",
    "            # Display the wordcloud\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.imshow(wc, interpolation='bilinear')\n",
    "            plt.axis('off')\n",
    "            plt.title(name)\n",
    "            plt.show()       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070c1f3f",
   "metadata": {},
   "source": [
    "## What topics appear the most in negative and successful videos? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987eeb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the video text field that we will use for the wordclouds ('title', 'tags' or 'description')\n",
    "field_to_analyze = 'description' \n",
    " \n",
    "def remove_stopwords(df):\n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned['tokens'] = df_cleaned[field_to_analyze].apply(lambda title: title.split())\n",
    "    stop_words = stopwords.words('english')\n",
    "    df_cleaned['tokens'] = df_cleaned['tokens'].apply(lambda tokens: [token for token in tokens if token.lower() not in stop_words])\n",
    "    return df_cleaned\n",
    "\n",
    "df_cleaned_lda = remove_stopwords(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ac36c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[Dont forget the put the reference]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff26a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category of videos where we will do topic extraction [[Djian: What category to choose? Or do the analysis for all categories together?]]\n",
    "#category_for_lda = 'Education'\n",
    "#category_for_lda = 'Entertainment'\n",
    "category_for_lda = 'Pets & Animals'\n",
    "#category_for_lda = 'Howto & Style'\n",
    "#category_for_lda = 'Travel & Events'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649eaf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics = df_cleaned_lda[df_cleaned_lda['categories'] == category_for_lda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c277d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url_and_tags(text):\n",
    "    text = re.sub(r'https?:\\S*','',text)\n",
    "    return re.sub(r'@\\S*','',text)\n",
    "df_topics.description = df_topics.description.apply(remove_url_and_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d2e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    lemmatizer = WordNetLemmatizer() # For Lemmatization\n",
    "    lem_words = []\n",
    "    for text in df['description']:\n",
    "        words=[w for w in nltk.tokenize.word_tokenize(text) if (w not in stopwords_set)]\n",
    "        words=[lemmatizer.lemmatize(w) for w in words if len(w)>2]\n",
    "        lem_words.append(words)\n",
    "    return lem_words\n",
    "\n",
    "words = preprocessing(df_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2938e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to gensim dictionary\n",
    "id2word = gensim.corpora.Dictionary(words) \n",
    "bows = [id2word.doc2bow(doc) for doc in words]\n",
    "#pickle.dump(bows, open('corpus.pkl', 'wb'))\n",
    "id2word.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14310453",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus=bows, num_topics=15, id2word=id2word, passes=10, workers=1, random_state=6)\n",
    "lda_model.save('model15.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92846c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We print words occuring in each of the topics as we iterate through them\n",
    "#for idx, topic in lda_model.print_topics(num_words=50):    \n",
    "#    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6bb31d",
   "metadata": {},
   "source": [
    "### Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af23a985",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loading the dictionary and corpus files we already saved\n",
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "\n",
    "# Loading the model of 15 categories we already saved\n",
    "lda_model = gensim.models.ldamodel.LdaModel.load('model15.gensim')\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, bows, id2word, sort_topics=False)\n",
    "\n",
    "# Export to HTML\n",
    "pyLDAvis.save_html(data=vis, fileobj='generated/topics.html')\n",
    "\n",
    "# Display\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8907f655",
   "metadata": {},
   "source": [
    "## Evolution of channels with negativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502dc18e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f3dfe788cd8a176cde1aa561c5672cfaa757d2e94ba5f857d8b94b70ba9c18d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
