{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0542057",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008e6057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import ijson\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f145d897",
   "metadata": {},
   "source": [
    "The file `yt_metadata_en.jsonl.gz` should be unzipped (~ 98 Go) in the data directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac76a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f865bbe3",
   "metadata": {},
   "source": [
    "### Process the big json file to small CSVs (one for each year) and then parquet\n",
    "\n",
    "Store the columns we need in `yt_metadata_en.jsonl` (all but `title`, `tags`, `description`) in a separate CSV file for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464fe3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Years from 2005 to 2019 (included)\n",
    "\n",
    "#years = [str(year) for year in range(2005, 2020)]\n",
    "years = ['2019'] # We only use 2019 at the moment, for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d5c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create CSVs with only the columns to keep (drop title, description and tags)\n",
    "\n",
    "f = open(data_path + 'yt_metadata_en.jsonl')\n",
    "videos = ijson.items(f, '', multiple_values=True)\n",
    "\n",
    "output = {year: open(f'generated/{year}/{year}_videos_few_columns.csv', 'w') for year in years}\n",
    "\n",
    "columns_to_drop = [\n",
    "    'description', \n",
    "    'tags', \n",
    "    'title'\n",
    "]\n",
    "columns_to_keep = [\n",
    "    'categories', \n",
    "    'channel_id', \n",
    "    'crawl_date', \n",
    "    'dislike_count', \n",
    "    'display_id', \n",
    "    'duration', \n",
    "    'like_count',\n",
    "    'upload_date',\n",
    "    'view_count'\n",
    "\n",
    "]\n",
    "columns = [f'\"{str(col)}\"' for col in columns_to_keep]\n",
    "\n",
    "# Write the headers in neach CSV\n",
    "for year in years:\n",
    "    output[year].write(','.join(columns))\n",
    "    output[year].write('\\n')\n",
    "\n",
    "# Put each video from the json file in the CSV with the correct year\n",
    "for video in tqdm(videos):\n",
    "    data_to_write = [str(video[field]) for field in columns_to_keep]\n",
    "    date_video = str(video['upload_date'])\n",
    "    year_video = date_video[:4]\n",
    "    \n",
    "    written = False\n",
    "    for year in years:\n",
    "        if year == year_video:\n",
    "            output[year].write(','.join(data_to_write))\n",
    "            output[year].write('\\n')\n",
    "            written = True\n",
    "            break\n",
    "    \n",
    "print('Finished') # 9min50s\n",
    "for year in years:\n",
    "    output[year].close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c78efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Convert each CSV file to a parquet file (faster to read)\n",
    "types_col = {    \n",
    "    'categories': 'string', \n",
    "    'channel_id': 'string', \n",
    "    'dislike_count': 'Int32',\n",
    "    'display_id': 'string', \n",
    "    'duration': 'string', \n",
    "    'like_count': 'Int64',\n",
    "    'view_count': 'Int64'\n",
    "}\n",
    "for year in tqdm(years):\n",
    "    v = pd.read_csv(\n",
    "        f'generated/{year}/{year}_videos_few_columns.csv', \n",
    "        sep=',', \n",
    "        header=0, \n",
    "        dtype=types_col, \n",
    "        parse_dates=['crawl_date', 'upload_date'],\n",
    "        na_values=['None'],\n",
    "        engine='c'\n",
    "    ) # 6min36s [[Djian: It might be faster using pyarrow csv]]\n",
    "    v.to_parquet(f'generated/{year}/{year}_videos_few_columns.parquet', compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecc9445",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos = {\n",
    "    year: pd.read_parquet(f'generated/{year}/{year}_videos_few_columns.parquet', engine='fastparquet') \n",
    "    for year in years\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a73f88",
   "metadata": {},
   "source": [
    "### Split the big json into CSV files (one for each year, keeping all columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c3545e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create CSVs that split the big json file into years\n",
    "\n",
    "f = open(data_path + 'yt_metadata_en.jsonl')\n",
    "videos = ijson.items(f, '', multiple_values=True)\n",
    "\n",
    "output = {year: open(f'generated/{year}/{year}_videos.csv', 'w') for year in years}\n",
    "\n",
    "columns_to_drop = [\n",
    "    'description', \n",
    "    'tags', \n",
    "    'title'\n",
    "]\n",
    "columns_to_keep = [\n",
    "    'categories', \n",
    "    'channel_id', \n",
    "    'crawl_date', \n",
    "    'dislike_count', \n",
    "    'display_id', \n",
    "    'duration', \n",
    "    'like_count',\n",
    "    'upload_date',\n",
    "    'view_count'\n",
    "\n",
    "]\n",
    "\n",
    "columns = columns_to_keep + columns_to_drop\n",
    "\n",
    "writer = {year: csv.DictWriter(output[year], fieldnames=columns) for year in years}\n",
    "\n",
    "for year in years:\n",
    "    writer[year].writeheader()\n",
    "\n",
    "# Put each video from the json file in the CSV with the correct year\n",
    "for video in tqdm(videos):\n",
    "    data_to_write = [str(video[field]) for field in columns_to_keep + columns_to_drop]\n",
    "    date_video = str(video['upload_date'])\n",
    "    year_video = date_video[:4]\n",
    "    \n",
    "    written = False\n",
    "    for year in years:\n",
    "        if year == year_video:\n",
    "            writer[year].writerow(video)\n",
    "    \n",
    "    \n",
    "print('Finished') # 9min50s\n",
    "for year in years:\n",
    "    output[year].close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dafb84",
   "metadata": {},
   "source": [
    "### Some helper functions for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad086989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to count the number of negative words in a text\n",
    "\n",
    "# Load the dataset of negative words\n",
    "neg_words = set(open('negative-words.txt', mode='r', encoding='iso-8859-1').read().strip().split(\"\\n\"))\n",
    "\n",
    "def count_neg_words(text, fieldname=''):\n",
    "    ''' Count the number of words and the number of negative words in the text\n",
    "    \n",
    "        :param text: a string\n",
    "        :param field: the name of the field\n",
    "        \n",
    "        :return: dictionary of features (nb_words, nb_negative_words)\n",
    "    '''\n",
    "    words = set(word.lower() for word in text.split(' '))\n",
    "    nb_negative = len(words.intersection(neg_words))\n",
    "    nb_words = len(words)\n",
    "    d =  {\n",
    "        f'count_words_{fieldname}': nb_words,\n",
    "        f'count_negative_words_{fieldname}': nb_negative\n",
    "    }\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c227aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for sentiment analysis\n",
    "\n",
    "# Load VADER from nltk\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment(text, fieldname=''):\n",
    "    ''' Perform sentiment analysis\n",
    "    \n",
    "        :param text: a string\n",
    "        :param field: the name of the field\n",
    "        \n",
    "        :return: dictionary of features (negative, neutral, positive, compound)\n",
    "    '''\n",
    "    \n",
    "    negative, neutral, positive, compound = sia.polarity_scores(text).values()\n",
    "    d = {\n",
    "        f'sia_negative_{fieldname}': negative,\n",
    "        f'sia_neutral_{fieldname}': neutral,\n",
    "        f'sia_positive_{fieldname}': positive,\n",
    "        f'sia_compound_{fieldname}': compound\n",
    "    }\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb1fc4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_features(text_to_features, year, field):\n",
    "    ''' Extract the video features according to a specified function, on a given year, on a given field.\n",
    "        \n",
    "        :param text_to_features: function that maps a string to a list of features. The prototype must be\n",
    "            text_to_features(text, fieldname) -> dict of features.\n",
    "        :param year: string of the year\n",
    "        :param field: name of the video field to analyse (string)\n",
    "        \n",
    "        :return: DataFrame with the features (each row corresponds to a video, the features are columns)\n",
    "    '''\n",
    "    \n",
    "    print('Computing features')\n",
    "    features_list = [] # list of dicts\n",
    "    with open(f'generated/{year}/{year}_videos.csv', \"r\") as f:\n",
    "        reader = csv.DictReader(f, delimiter=\",\")\n",
    "        for video in tqdm(reader):\n",
    "            features_video = text_to_features(video[field], fieldname=field)\n",
    "            features_video\n",
    "            features_list.append(features_video)\n",
    "    print('...done.')\n",
    "\n",
    "    print('Converting features to dataframe...')\n",
    "    features_list = pd.DataFrame.from_dict(features_list)\n",
    "    print('...done.')\n",
    "    \n",
    "    return features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176f0e89",
   "metadata": {},
   "source": [
    "### Extract the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e0d90f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count negative words in titles in 2019\n",
    "df_neg_words_title = extract_features(text_to_features=count_neg_words, year='2019', field='title') # 3min22s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f68bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count negative words in descriptions in 2019\n",
    "df_neg_words_desc = extract_features(text_to_features=count_neg_words, year='2019', field='description') # 6min30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed7a662",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sentiment analysis on titles in 2019\n",
    "df_sia = extract_features(text_to_features=sentiment, year='2019', field='title') # 20min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbf6da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis on descriptions in 2019\n",
    "#df_sia_desc = extract_features(text_to_features=sentiment, year='2019', field='description') # ~3h (projection) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bfac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all the dataframes of features\n",
    "df_features = df_neg_words_title.join(df_neg_words_desc).join(df_sia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f68c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the df of videos from 2019 (without titles, tags and descriptions, to have a small file)\n",
    "videos_few_cols = pd.read_parquet('generated/2019/2019_videos_few_columns.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210280dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the features to the videos\n",
    "videos_features = videos_few_cols.join(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328eb192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the df videos with features to a parquet file\n",
    "videos_features.to_parquet(f'generated/2019/2019_videos_CountNegWords_Sentiment.parquet', compression=None) # 3s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24435de3",
   "metadata": {},
   "source": [
    "### How to load the dataframe with videos and features (from 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c140c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run:\n",
    "videos_features = pd.read_parquet('generated/2019/2019_videos_CountNegWords_Sentiment.parquet', engine='fastparquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
