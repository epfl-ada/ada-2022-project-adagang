{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0542057",
   "metadata": {},
   "source": [
    "# Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c866c",
   "metadata": {},
   "source": [
    "Here we process the data to obtain a smaller file that is easier to work with.\n",
    "\n",
    "This process is very long. It took 3-4 hours on an Intel i7-1165G7 processor at 2.80 GHz using a single core, with 64 GB of RAM (with swap space) and around 200 GB of disk space.\n",
    "\n",
    "The file `yt_metadata_en.jsonl.gz` from Zenodo (https://zenodo.org/record/4650046) should be unzipped (~ 98 Go) in the data directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008e6057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import ijson\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from helpers import count_upper_and_excl, count_neg_emojis, count_neg_words, sentiment, extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac76a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f865bbe3",
   "metadata": {},
   "source": [
    "### Process the big json file to small CSVs (one for each year) and then parquet\n",
    "\n",
    "Store the columns we need in `yt_metadata_en.jsonl` (all but `title`, `tags`, `description`) in a separate CSV file for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464fe3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Years from 2005 to 2019 (included)\n",
    "\n",
    "#years = [str(year) for year in range(2005, 2020)]\n",
    "years = ['2019'] # We only use 2019 at the moment, for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d5c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create CSVs with only the columns to keep (drop title, description and tags)\n",
    "\n",
    "f = open(data_path + 'yt_metadata_en.jsonl')\n",
    "videos = ijson.items(f, '', multiple_values=True)\n",
    "\n",
    "output = {year: open(f'generated/{year}/{year}_videos_few_columns.csv', 'w') for year in years}\n",
    "\n",
    "columns_to_drop = [\n",
    "    'description', \n",
    "    'tags', \n",
    "    'title'\n",
    "]\n",
    "columns_to_keep = [\n",
    "    'categories', \n",
    "    'channel_id', \n",
    "    'crawl_date', \n",
    "    'dislike_count', \n",
    "    'display_id', \n",
    "    'duration', \n",
    "    'like_count',\n",
    "    'upload_date',\n",
    "    'view_count'\n",
    "\n",
    "]\n",
    "columns = [f'\"{str(col)}\"' for col in columns_to_keep]\n",
    "\n",
    "# Write the headers in neach CSV\n",
    "for year in years:\n",
    "    output[year].write(','.join(columns))\n",
    "    output[year].write('\\n')\n",
    "\n",
    "# Put each video from the json file in the CSV with the correct year\n",
    "for video in tqdm(videos):\n",
    "    data_to_write = [str(video[field]) for field in columns_to_keep]\n",
    "    date_video = str(video['upload_date'])\n",
    "    year_video = date_video[:4]\n",
    "    \n",
    "    written = False\n",
    "    for year in years:\n",
    "        if year == year_video:\n",
    "            output[year].write(','.join(data_to_write))\n",
    "            output[year].write('\\n')\n",
    "            written = True\n",
    "            break\n",
    "    \n",
    "print('Finished') # 9min50s\n",
    "for year in years:\n",
    "    output[year].close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c78efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Convert each CSV file to a parquet file (faster to read)\n",
    "types_col = {    \n",
    "    'categories': 'string', \n",
    "    'channel_id': 'string', \n",
    "    'dislike_count': 'Int32',\n",
    "    'display_id': 'string', \n",
    "    'duration': 'string', \n",
    "    'like_count': 'Int64',\n",
    "    'view_count': 'Int64'\n",
    "}\n",
    "for year in tqdm(years):\n",
    "    v = pd.read_csv(\n",
    "        f'generated/{year}/{year}_videos_few_columns.csv', \n",
    "        sep=',', \n",
    "        header=0, \n",
    "        dtype=types_col, \n",
    "        parse_dates=['crawl_date', 'upload_date'],\n",
    "        na_values=['None'],\n",
    "        engine='c'\n",
    "    ) # 6min36s [[Djian: It might be faster using pyarrow csv]]\n",
    "    v.to_parquet(f'generated/{year}/{year}_videos_few_columns.parquet', compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecc9445",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos = {\n",
    "    year: pd.read_parquet(f'generated/{year}/{year}_videos_few_columns.parquet', engine='fastparquet') \n",
    "    for year in years\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a73f88",
   "metadata": {},
   "source": [
    "### Split the big json into CSV files (one for each year, keeping all columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c3545e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create CSVs that split the big json file into years\n",
    "\n",
    "f = open(data_path + 'yt_metadata_en.jsonl')\n",
    "videos = ijson.items(f, '', multiple_values=True)\n",
    "\n",
    "output = {year: open(f'generated/{year}/{year}_videos.csv', 'w') for year in years}\n",
    "\n",
    "columns_to_drop = [\n",
    "    'description', \n",
    "    'tags', \n",
    "    'title'\n",
    "]\n",
    "columns_to_keep = [\n",
    "    'categories', \n",
    "    'channel_id', \n",
    "    'crawl_date', \n",
    "    'dislike_count', \n",
    "    'display_id', \n",
    "    'duration', \n",
    "    'like_count',\n",
    "    'upload_date',\n",
    "    'view_count'\n",
    "]\n",
    "\n",
    "columns = columns_to_keep + columns_to_drop\n",
    "\n",
    "writer = {year: csv.DictWriter(output[year], fieldnames=columns) for year in years}\n",
    "\n",
    "for year in years:\n",
    "    writer[year].writeheader()\n",
    "\n",
    "# Put each video from the json file in the CSV with the correct year\n",
    "for video in tqdm(videos):\n",
    "    data_to_write = [str(video[field]) for field in columns_to_keep + columns_to_drop]\n",
    "    date_video = str(video['upload_date'])\n",
    "    year_video = date_video[:4]\n",
    "    \n",
    "    written = False\n",
    "    for year in years:\n",
    "        if year == year_video:\n",
    "            writer[year].writerow(video)\n",
    "    \n",
    "print('Finished') # 9min50s\n",
    "for year in years:\n",
    "    output[year].close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176f0e89",
   "metadata": {},
   "source": [
    "### Extract the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ef5de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [] # to be filled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b581cbaa",
   "metadata": {},
   "source": [
    "#### Count uppercase words and exclamation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dab557e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count in titles in 2019\n",
    "df_typography_title = extract_features(text_to_features=count_upper_and_excl, year='2019', field='title')\n",
    "features.append(df_typography_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944eb340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count in descriptions in 2019\n",
    "df_typography_desc = extract_features(text_to_features=count_upper_and_excl, year='2019', field='description')\n",
    "features.append(df_typography_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66470915",
   "metadata": {},
   "source": [
    "#### Count negative emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdab9157",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count in titles in 2019\n",
    "df_emojis_title = extract_features(text_to_features=count_neg_emojis, year='2019', field='title')\n",
    "features.append(df_emojis_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af36fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count in descriptions in 2019\n",
    "df_emojis_desc = extract_features(text_to_features=count_neg_emojis, year='2019', field='description')\n",
    "features.append(df_emojis_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e193f8c",
   "metadata": {},
   "source": [
    "#### Count negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e0d90f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count in titles in 2019\n",
    "df_neg_words_title = extract_features(text_to_features=count_neg_words, year='2019', field='title') # 3min22s\n",
    "features.append(df_neg_words_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f68bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count in descriptions in 2019\n",
    "df_neg_words_desc = extract_features(text_to_features=count_neg_words, year='2019', field='description') # 6min30s\n",
    "features.append(df_neg_words_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ea82b",
   "metadata": {},
   "source": [
    "#### Compute sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed7a662",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sentiment on titles in 2019\n",
    "df_sia_title = extract_features(text_to_features=sentiment, year='2019', field='title') # 20min\n",
    "features.append(df_sia_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbf6da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment on descriptions in 2019\n",
    "df_sia_desc = extract_features(text_to_features=sentiment, year='2019', field='description') # 2h51min\n",
    "features.append(df_sia_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e19047",
   "metadata": {},
   "source": [
    "#### Join the dataframes and store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bfac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all the dataframes of features\n",
    "df_features = features[0]\n",
    "for i in range(1, len(features)):\n",
    "    df_features = df_features.join(features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f68c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the df of videos from 2019 (without titles, tags and descriptions, to have a small file)\n",
    "videos_few_cols = pd.read_parquet('generated/2019/2019_videos_few_columns.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210280dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the features to the videos\n",
    "videos_features = videos_few_cols.join(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328eb192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the df videos with features to a parquet file\n",
    "videos_features.to_parquet(f'generated/2019/2019_videos_Typo_Emojis_NegWords_Sentiment_title_desc.parquet', compression=None) # 3s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24435de3",
   "metadata": {},
   "source": [
    "#### How to load the dataframe with videos and features (from 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c140c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run:\n",
    "videos_features = pd.read_parquet('generated/2019/2019_videos_Typo_Emojis_NegWords_Sentiment_title_desc.parquet', engine='fastparquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f3dfe788cd8a176cde1aa561c5672cfaa757d2e94ba5f857d8b94b70ba9c18d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
