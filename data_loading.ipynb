{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0542057",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008e6057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f145d897",
   "metadata": {},
   "source": [
    "The file `yt_metadata_en.jsonl.gz` should be unzipped (~ 98 Go) in the data directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac76a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f865bbe3",
   "metadata": {},
   "source": [
    "## Process the big json file to a csv\n",
    "\n",
    "Store the columns we need in `yt_metadata_en.jsonl` (all but `title`, `tags`, `description`) in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d5c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a csv with only the columns to keep (drop title, description and tags)\n",
    "\n",
    "f = open(data_path + 'yt_metadata_en.jsonl')\n",
    "output = open('generated/videos_few_columnstmp.csv', 'w')\n",
    "\n",
    "videos = ijson.items(f, '', multiple_values=True)\n",
    "\n",
    "columns_to_drop = [\n",
    "    'description', \n",
    "    'tags', \n",
    "    'title'\n",
    "]\n",
    "    \n",
    "columns_to_keep = [\n",
    "    'categories', \n",
    "    'channel_id', \n",
    "    'crawl_date', \n",
    "    'dislike_count', \n",
    "    'display_id', \n",
    "    'duration', \n",
    "    'like_count',\n",
    "    'upload_date',\n",
    "    'view_count'\n",
    "\n",
    "]\n",
    "\n",
    "columns = [f'\"{str(col)}\"' for col in columns_to_keep]\n",
    "output.write(','.join(columns))\n",
    "output.write('\\n')\n",
    "\n",
    "for video in tqdm(videos):\n",
    "    \n",
    "    data_to_write = [str(video[field]) for field in columns_to_keep]\n",
    "    output.write(','.join(data_to_write))\n",
    "    output.write('\\n')\n",
    "    \n",
    "    \n",
    "print('Finished') # 11min05s\n",
    "output.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c78efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "types_col = {    \n",
    "    'categories': 'string', \n",
    "    'channel_id': 'string', \n",
    "    'crawl_date': 'string', \n",
    "    'dislike_count': 'Int32',\n",
    "    'display_id': 'string', \n",
    "    'duration': 'string', \n",
    "    'like_count': 'Int64',\n",
    "    'upload_date': 'string', \n",
    "    'view_count': 'Int64'\n",
    "}\n",
    "v = pd.read_csv(\n",
    "    'generated/videos_few_columns.csv', \n",
    "    sep=',', \n",
    "    header=0, \n",
    "    dtype=types_col, \n",
    "    na_values=['None'], \n",
    "    engine='c'\n",
    ") # 6min36s [[Djian: It might be faster using pyarrow csv]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a866f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "v.to_parquet('generated/videos_few_columns.parquet', compression=None) # 4min14s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b843611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w = pd.read_parquet('generated/videos_few_columns.parquet', engine='fastparquet') # 37s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa3d353",
   "metadata": {},
   "source": [
    "## Count negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed80ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to count the number of negative words in a text\n",
    "\n",
    "neg_words = set(open('negative-words.txt', mode='r', encoding='iso-8859-1').read().strip().split(\"\\n\"))\n",
    "\n",
    "def count_neg_words(text):\n",
    "    words = set(word.lower() for word in text.split(' '))\n",
    "    nb_negative = len(words.intersection(neg_words))\n",
    "    nb_words = len(words)\n",
    "    return nb_negative, nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efabba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the negative words in titles (output: list of dictionaries, one dict for each video)\n",
    "\n",
    "f = open(data_path + 'yt_metadata_en.jsonl')\n",
    "videos = ijson.items(f, '', multiple_values=True)\n",
    "\n",
    "list_new_data = []\n",
    "\n",
    "for video in tqdm(videos):    \n",
    "    \n",
    "    # Count negative words in title\n",
    "    count_neg_words_title, count_words_title = count_neg_words(video['title'])\n",
    "    \n",
    "    list_new_data.append({\n",
    "        'count_words_title': count_words_title,\n",
    "        'count_negative_words_title': count_neg_words_title\n",
    "    })\n",
    "    \n",
    "print('Finished') # 11min26s\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8164784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dicts to a dataframe\n",
    "%%time\n",
    "newcols = pd.DataFrame.from_dict(list_new_data) # ~5min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7fc1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Join the dataframe of videos with the newly copmuted columns\n",
    "%%time\n",
    "joined = w.join(newcols) # 1min53s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972c2ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the joined dataframe to a file\n",
    "%%time \n",
    "joined.to_parquet('generated/videos_CountNegWordsTitle.parquet', compression=None) # 3min35s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ad2e37",
   "metadata": {},
   "source": [
    "## Sentiment analysis computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8cd425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sentiment analysis in titles (output: list of dictionaries, one dict for each video)\n",
    "\n",
    "f = open(data_path + 'yt_metadata_en.jsonl')\n",
    "videos = ijson.items(f, '', multiple_values=True)\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "list_new_data = []\n",
    "for video in tqdm(videos):    \n",
    "    \n",
    "    # Compute sentiment for the title\n",
    "    negative, neutral, positive, compound = sia.polarity_scores(video['title']).values()\n",
    "\n",
    "    list_new_data.append({\n",
    "        'sia_negative': negative,\n",
    "        'sia_neutral': neutral,\n",
    "        'sia_positive': positive,\n",
    "        'sia_compound': compound\n",
    "    })\n",
    "    \n",
    "print('Finished') # [[Djian: It will probably take a bit less than 2 hours]]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60169281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[...]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
